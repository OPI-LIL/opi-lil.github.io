---
layout: post
title: "SBT Apache Spark Plugin"
description: "SBT plugin for easy deployment and submission to remote linux machine running apache spark"
category: [NLP]
tags: [NLP, Text Mining]
---

In the [Natural Language Processing Laboratory](http://www.opi.org.pl/en/Natural-Language-Processing-Lab.html) we are making extensive use of [Scala programming language](http://www.scala-lang.org/) and [Apache Spark](https://spark.apache.org/) (blazingly fast and general engine for large-scale data processing). To make our life easier we have decided to write a simple, yet really usefull sbt plugin easing the burden of deployment and submission.  Thanks to [sbt-spark](https://github.com/OPI-LIL/sbt-spark) plugin one can easily submit  jars to [Apache Spark](https://spark.apache.org/) from his/hers local machine. 

Thanks to the courtesy of [sbt organization](http://www.scala-sbt.org/) [sbt-spark](https://github.com/OPI-LIL/sbt-spark) plugin has been included in the [community sbt repository](https://bintray.com/sbt/sbt-plugin-releases) makig it available for all sbt users without additonal configuration.

<!--more-->
## Installation
So what should one do, to actually use [sbt-spark](https://github.com/OPI-LIL/sbt-spark) plugin? The only requirement is that the following line is added to the sbt project directory: `project/sbt-spark.sbt` file:

```scala
addSbtPlugin("com.opi.lil" % "sbt-spark" % "0.1.7")
```

That's it.

## Usage

Now in your `build.sbt` file you should include the following

```scala
scalaVersion := "2.10.4"

lazy val commonSettings = Seq(
  organization := "com.opi.lil",              // your organization name
  version := "1.0",                           // your project version
  scalaVersion := "2.10.4",                   // your scala version
  user := "user",                             // your username on host
  host := "0.0.0.0",                          // your host adress  
  key := "PATH_TO_YOUR_PRIVATE_KEY",          // path to private key in OpenSSH format
  destFolder := "/home/spark/dev/",           // dest directory of jar file
  defaultClass := "MainApp"                   // class to be submitted to apache spark
)

lazy val root = (project in file(".")).
  settings(commonSettings: _*).
  settings(
  	name := "spark-test",  
    libraryDependencies ++= Seq(
      "org.apache.spark"  % "spark-core_2.10"  % "1.2.0", 
      "org.apache.spark"  % "spark-mllib_2.10" % "1.2.0",
      "org.apache.hadoop" % "hadoop-client" % "2.5.0",
      "com.typesafe" % "config" % "1.0.0")  
  )
```

In order to deoploy project jar file to the remote server specified in `build.sbt` configuration file, one has to to use `deploy` command in sbt command line:

    > deploy

To submit given class to Apache Spark type `submit ClassName`   

    > submit ClassName
  
If the class name has not been explicitly stated, the class defined in the `defaultClass`  `settingKey` will be used. 

## Minimal Working Example
Minimal Working Example can be found in our [github repository](https://github.com/OPI-LIL/apache-spark-template).
 
To start just clone the sample project:

	 > git clone https://github.com/OPI-LIL/apache-spark-template.git sbt-spark-test

... and submit it to Apache Spark:

	> sbt submit